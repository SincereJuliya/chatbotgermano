{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EimVurhQ45yk"
      },
      "source": [
        "# Introduction to Retrival Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "621x3COOrKdG"
      },
      "source": [
        "RAG solution gives you an LLM that can provide the right responses to the user in different scenarios. A good RAG system should generate a grounded response based on relevant documents, but it should not do that every single time. The system also has to be able to determine whether or not any of the provided documents are relevant (and possibly decide that none are relevant), as well as decide that it can directly respond without needing any documents retrieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jBEKety4mKZ"
      },
      "source": [
        "The chatbot can extract relevant information from external documents and produce verifiable, inline citations in its responses.\n",
        "\n",
        "The diagram below provides an overview of what we’ll build."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIs9ggAVSZdR"
      },
      "source": [
        "<img src=\"https://github.com/cohere-ai/cohere-developer-experience/blob/main/notebooks/images/llmu/rag/rag-workflow-1.png?raw=1\" alt=\"Workflow\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qci1A1RNWSU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# install dotenv if not already installed\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ImportError:\n",
        "    os.system('pip install python-dotenv')\n",
        "    from dotenv import load_dotenv\n",
        "    \n",
        "load_dotenv()\n",
        "\n",
        "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVhenxwXE9oI",
        "outputId": "8db44ffa-f649-4a20-fe1d-ff060ee8ac04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# ! pip install cohere hnswlib unstructured -q # hnswlib for the vector library, unstructured for chunking the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "pkljD2E9xF6r"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "# import uuid\n",
        "import hnswlib\n",
        "from typing import List, Dict\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiAF_FuHF2Mg"
      },
      "outputs": [],
      "source": [
        "#@title Enable text wrapping in Google Colab\n",
        "\n",
        "# from IPython.display import HTML, display\n",
        "\n",
        "# def set_css():\n",
        "#   display(HTML('''\n",
        "#   <style>\n",
        "#     pre {\n",
        "#         white-space: pre-wrap;\n",
        "#     }\n",
        "#   </style>\n",
        "#   '''))\n",
        "# get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2gY7O7rqE6Nq",
        "outputId": "87fe110e-27ee-4f5d-a3fb-b437a7376bfd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "co = cohere.Client(COHERE_API_KEY) # Get your API key here: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCcnfXci4yGl"
      },
      "source": [
        "## Simple Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLXxYR_UHGLJ"
      },
      "source": [
        "### Define documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKbrq5c1e45y"
      },
      "source": [
        "We define the documents that we want to ground an LLM’s response with, formatted as a list. In our case, each document consists of two fields: title and text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CdxeI3XW4yIH",
        "outputId": "8a041045-fc09-4211-9d35-2212abe00f37"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    {\n",
        "        \"title\": \"Tall penguins\",\n",
        "        \"text\": \"Emperor penguins are the tallest.\"},\n",
        "    {\n",
        "        \"title\": \"Penguin habitats\",\n",
        "        \"text\": \"Emperor penguins only live in Antarctica.\"},\n",
        "    {\n",
        "        \"title\": \"What are animals?\",\n",
        "        \"text\": \"Animals are different from plants.\"},\n",
        "    {\n",
        "        \"title\": \"AI student\",\n",
        "        \"text\": \"Yusuf is an AI student\"},\n",
        "    {\n",
        "        \"title\": \"AI student home Univerity name\",\n",
        "        \"text\": \"Yusuf studies at La Sapienza Univeristy\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"AI student host Univerity location\",\n",
        "        \"text\": \"He is currently doing an Erasmus Programme in Trento Unversity\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Location of Home University and Host Univeristy\",\n",
        "        \"text\": \"La Sapienza is a university located in Rome in the Lazio region of Italy. Trento university, instead, is located in Trento in Italy\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT3KjYATHH9o"
      },
      "source": [
        "### Generate response with citations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A64u2fIyfBri"
      },
      "source": [
        "\n",
        "First, we define the user message. Then we generate the response from the LLM and display it, together with citations and the source documents used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "fkZ6gEYGISWZ",
        "outputId": "890c2e98-ad5f-4506-812b-ba6a8a7fc1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm doing well, thanks for asking.\n",
            "\n",
            "The tallest living penguins are Emperor penguins.\n",
            "\n",
            "Yusuf is an AI student who studies at La Sapienza University. His host university, Trento University, is located in Trento, Italy.\n",
            "\n",
            "CITATIONS:\n",
            "start=68 end=85 text='Emperor penguins.' document_ids=['doc_0'] type='TEXT_CONTENT'\n",
            "start=99 end=109 text='AI student' document_ids=['doc_3'] type='TEXT_CONTENT'\n",
            "start=125 end=148 text='La Sapienza University.' document_ids=['doc_4'] type='TEXT_CONTENT'\n",
            "start=170 end=187 text='Trento University' document_ids=['doc_5'] type='TEXT_CONTENT'\n",
            "start=203 end=217 text='Trento, Italy.' document_ids=['doc_6'] type='TEXT_CONTENT'\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Emperor penguins are the tallest.', 'title': 'Tall penguins'}\n",
            "{'id': 'doc_3', 'text': 'Yusuf is an AI student', 'title': 'AI student'}\n",
            "{'id': 'doc_4', 'text': 'Yusuf studies at La Sapienza Univeristy', 'title': 'AI student home Univerity name'}\n",
            "{'id': 'doc_5', 'text': 'He is currently doing an Erasmus Programme in Trento Unversity', 'title': 'AI student host Univerity location'}\n",
            "{'id': 'doc_6', 'text': 'La Sapienza is a university located in Rome in the Lazio region of Italy. Trento university, instead, is located in Trento in Italy', 'title': 'Location of Home University and Host Univeristy'}\n"
          ]
        }
      ],
      "source": [
        "# Get the user message\n",
        "message = \"Hi? How are doing? What are the tallest living penguins? Who is Yusuf? Where is his host university located?\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat_stream(message=message,\n",
        "                          model=\"command-a-03-2025\",\n",
        "                          documents=documents)\n",
        "\n",
        "# Display the response\n",
        "citations = []\n",
        "cited_documents = []\n",
        "\n",
        "for event in response:\n",
        "    if event.event_type == \"text-generation\":\n",
        "        print(event.text, end=\"\")\n",
        "    elif event.event_type == \"citation-generation\":\n",
        "        citations.extend(event.citations)\n",
        "    elif event.event_type == \"stream-end\":\n",
        "      cited_documents = event.response.documents\n",
        "\n",
        "# Display the citations and source documents\n",
        "if citations:\n",
        "  print(\"\\n\\nCITATIONS:\")\n",
        "  for citation in citations:\n",
        "    print(citation)\n",
        "\n",
        "  print(\"\\nDOCUMENTS:\")\n",
        "  for document in cited_documents:\n",
        "    print(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yi5RBHI4-rE"
      },
      "source": [
        "## Level 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2NA4ASfHqr2"
      },
      "source": [
        "There are three RAG modes available with the Cohere Chat endpoint:\n",
        "\n",
        "- Document mode: Specifying the documents for the model to use when generating a response\n",
        "- Connectors mode: Connecting the endpoint with an external service that handles all the logic of document retrieval\n",
        "- Query-generation mode: Generating one or more queries given a user message\n",
        "\n",
        "Note that the Document Mode also includes the Query-generation Mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4i3uNiCIGog"
      },
      "source": [
        "![An overview of what we'll build](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Frag-workflow-2.png&w=2048&q=75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mABcHeUMJPnZ"
      },
      "source": [
        "The steps to building a RAG-powered chatbot are summarized below:\n",
        "\n",
        "0. Setup phase:\n",
        "   - Step 0: Ingest the documents – get documents, chunk, embed, and index\n",
        "\n",
        "1. For each user-chatbot interaction:\n",
        "   - Step 1: Get the user message\n",
        "   - Step 2: Call the Chat endpoint in query-generation mode\n",
        "   - If at least one query is generated:\n",
        "     - Step 3: Retrieve and rerank relevant documents\n",
        "     - Step 4: Call the Chat endpoint in document mode to generate a grounded response with citations\n",
        "   - If no query is generated:\n",
        "     - Step 4: Call the Chat endpoint in normal mode to generate a response\n",
        "\n",
        "   - Throughout the conversation:\n",
        "     - Append the user-chatbot interaction to the conversation thread\n",
        "     - Repeat with every interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "rWYQzxOj4-XQ",
        "outputId": "84b3a5ad-d46d-4b32-b01b-a09488170af0"
      },
      "outputs": [],
      "source": [
        "# @title Defining documents\n",
        "\n",
        "raw_documents = [\n",
        "    {\n",
        "        \"title\": \"Crafting Effective Prompts\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
        "    {\n",
        "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
        "    {\n",
        "        \"title\": \"Prompt Truncation\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
        "    {\n",
        "        \"title\": \"Preambles\",\n",
        "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnqS_6Ai56Hd"
      },
      "source": [
        "![Vectore Store](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F04%2Frag-components-vectorstore.png&w=2048&q=75)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgCPupxq6F00"
      },
      "source": [
        "![The document ingestion portion of the Documents component](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2024%2F03%2Frag-chatbot-embedding.png&w=2048&q=75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zJeSnUSW5dKO",
        "outputId": "0e9c8c62-70c5-42d1-eb65-6ba66efafa31"
      },
      "outputs": [],
      "source": [
        "# @title Vectorstore\n",
        "\n",
        "class Vectorstore:\n",
        "    \"\"\"The Vectorstore class handles the ingestion of documents into embeddings (or vectors)\n",
        "    and the retrieval of relevant documents given a query.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
        "        self.raw_documents = raw_documents\n",
        "        self.docs = [] # chunked version of the documents\n",
        "        self.docs_embs = [] # embeddings of the chunked documents\n",
        "        self.retrieve_top_k = 10\n",
        "        self.rerank_top_k = 3\n",
        "        self.load_and_chunk()\n",
        "        self.embed()\n",
        "        self.index()\n",
        "\n",
        "    def load_and_chunk(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads the text from the sources and chunks the HTML content.\n",
        "        \"\"\"\n",
        "        print(\"Loading documents...\")\n",
        "\n",
        "        for raw_document in self.raw_documents:\n",
        "            elements = partition_html(url=raw_document[\"url\"])\n",
        "            chunks = chunk_by_title(elements)\n",
        "            for chunk in chunks:\n",
        "                self.docs.append(\n",
        "                    {\n",
        "                        \"title\": raw_document[\"title\"],\n",
        "                        \"text\": str(chunk),\n",
        "                        \"url\": raw_document[\"url\"],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    def embed(self) -> None:\n",
        "        \"\"\"\n",
        "        Embeds the document chunks using the Cohere API.\n",
        "\n",
        "        With the Embed v3 model, we need to define an input_type, of which there are four options depending\n",
        "        on the type of task. Using these input types ensures the highest possible quality for the respective tasks.\n",
        "        Since our document chunks will be used for retrieval, we use search_document as the input_type\n",
        "        \"\"\"\n",
        "        print(\"Embedding document chunks...\")\n",
        "\n",
        "        # Since the endpoint has a limit of 96 documents per call, we send them in batches.\n",
        "        batch_size = 90\n",
        "        self.docs_len = len(self.docs)\n",
        "        for i in range(0, self.docs_len, batch_size):\n",
        "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
        "            texts = [item[\"text\"] for item in batch]\n",
        "            docs_embs_batch = co.embed(\n",
        "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
        "            ).embeddings\n",
        "            self.docs_embs.extend(docs_embs_batch)\n",
        "\n",
        "    def index(self) -> None:\n",
        "        \"\"\"\n",
        "        Indexes the documents for efficient retrieval.\n",
        "\n",
        "        For production environments, typically a vector database (like Weaviate or MongoDB) is\n",
        "        required to handle the continuous process of indexing documents and maintaining the index.\n",
        "\n",
        "        Here, however, we’ll keep it simple and use a vector library instead.\n",
        "        We can choose from many open-source projects, such as Faiss, Annoy, ScaNN,\n",
        "        or Hnswlib, which is the one we’ll use.\n",
        "        These libraries store embeddings in in-memory indexes and implement\n",
        "        approximate nearest neighbor (ANN) algorithms to make similarity search efficient.\n",
        "        \"\"\"\n",
        "        print(\"Indexing documents...\")\n",
        "\n",
        "        # ip = inner product for the similarity metric to be used\n",
        "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
        "\n",
        "        # ef_construction=512: Controls the quality and speed of index construction.\n",
        "        # Higher values lead to better recall at the cost of slower indexing.\n",
        "        # M=64: Determines the number of bi-directional links created for each element in the HNSW graph.\n",
        "        # Larger values increase accuracy but also increase memory usage.\n",
        "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
        "\n",
        "        # Add the embeddings to the index with their corresponding IDs from (0 to len(docs_embs))\n",
        "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
        "\n",
        "        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n",
        "\n",
        "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"Retrieves document chunks based on the given query using Semantic Search.\n",
        "        It has 2 steps: Dense retrieval and Reranking.\n",
        "\n",
        "        While our dense retrieval component is already highly capable of retrieving relevant sources,\n",
        "        Cohere Rerank provides an additional boost to the quality of the search results,\n",
        "        especially for complex and domain-specific queries. It takes the search results and\n",
        "        sorts them according to their relevance to the query.\n",
        "\n",
        "        Parameters:\n",
        "        query (str): The query to retrieve document chunks for.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
        "        \"\"\"\n",
        "\n",
        "        # Dense retrieval with input_type=”search_query” for queries\n",
        "        query_emb = co.embed(\n",
        "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
        "        ).embeddings\n",
        "\n",
        "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
        "        print(f\"Retrieved document IDs: {doc_ids}\")\n",
        "\n",
        "        # Reranking for additional boost in relevance\n",
        "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
        "\n",
        "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "        rerank_results = co.rerank(\n",
        "            query=query,\n",
        "            documents=docs_to_rerank,\n",
        "            top_n=self.rerank_top_k,\n",
        "            model=\"rerank-english-v3.0\",\n",
        "            rank_fields=rank_fields\n",
        "        )\n",
        "\n",
        "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
        "        print(f\"Rerank results: {rerank_results.results}\")\n",
        "\n",
        "        docs_retrieved = []\n",
        "        for i, doc_id in enumerate(doc_ids_reranked):\n",
        "            docs_retrieved.append(\n",
        "                {\n",
        "                    \"title\": self.docs[doc_id][\"title\"],\n",
        "                    \"text\": self.docs[doc_id][\"text\"],\n",
        "                    \"url\": self.docs[doc_id][\"url\"],\n",
        "                    \"id\": int(doc_id),\n",
        "                    \"relevance_score\": rerank_results.results[i].relevance_score,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        return docs_retrieved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "KbvaRoCcWaDL",
        "outputId": "485638e7-2df8-4e6b-b3fd-f435d689c755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading documents...\n",
            "Embedding document chunks...\n",
            "Indexing documents...\n",
            "Indexing complete with 120 documents.\n"
          ]
        }
      ],
      "source": [
        "# @title Process the Documents.\n",
        "# In our case, we get a total of 136 documents, chunked from the four web URLs.\n",
        "vectorstore = Vectorstore(raw_documents=raw_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "D3L-HFxUY23Z",
        "outputId": "dfdba299-6b94-4196-b729-d5cfdd6b5b8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved document IDs: [55 64 31  1 43 93 56 42 63 41]\n",
            "Rerank results: [RerankResponseResultsItem(document=None, index=0, relevance_score=0.99554896), RerankResponseResultsItem(document=None, index=2, relevance_score=0.98835784), RerankResponseResultsItem(document=None, index=6, relevance_score=0.96182173)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'title': 'Advanced Prompt Engineering Techniques',\n",
              "  'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',\n",
              "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques',\n",
              "  'id': 55,\n",
              "  'relevance_score': 0.99554896},\n",
              " {'title': 'Crafting Effective Prompts',\n",
              "  'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
              "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts',\n",
              "  'id': 31,\n",
              "  'relevance_score': 0.98835784},\n",
              " {'title': 'Advanced Prompt Engineering Techniques',\n",
              "  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',\n",
              "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques',\n",
              "  'id': 56,\n",
              "  'relevance_score': 0.96182173}]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title Testing Retrieval\n",
        "vectorstore.retrieve(\"Prompting by giving examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tCKQOyWyZFZe",
        "outputId": "5368e142-80b4-4b64-9e86-610c4df8f519"
      },
      "outputs": [],
      "source": [
        "# @title Run the Chatbot\n",
        "\n",
        "def run_chatbot(message, chat_history=[]):\n",
        "    \"\"\"\n",
        "    1. Give the user message to the LLM to determine if additional context is needed\n",
        "    2. If so:\n",
        "       - The LLM returns search queries\n",
        "       - We retrieve the documents from the DB,\n",
        "       - The LLM uses the documents as context and responds\n",
        "    3. If not:\n",
        "       - The LLM responds directly without additional context\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate search queries, if any\n",
        "    response = co.chat(message=message,\n",
        "                        model=\"command-a-03-2025\",\n",
        "                        search_queries_only=True, # Generate only search queries, not full responses\n",
        "                        chat_history=chat_history)\n",
        "\n",
        "    search_queries = []\n",
        "    for query in response.search_queries:\n",
        "        search_queries.append(query.text)\n",
        "\n",
        "    # If there are search queries, retrieve the documents\n",
        "    if search_queries:\n",
        "        print(\"Retrieving information...\", end=\"\")\n",
        "\n",
        "        # Retrieve document chunks for each query\n",
        "        documents = []\n",
        "        for query in search_queries:\n",
        "            documents.extend(vectorstore.retrieve(query))\n",
        "\n",
        "        # Use document chunks to respond\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-a-03-2025\",\n",
        "            documents=documents,\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # If no additional context is needed, respond directly\n",
        "        response = co.chat_stream(\n",
        "            message=message,\n",
        "            model=\"command-a-03-2025\",\n",
        "            chat_history=chat_history,\n",
        "        )\n",
        "\n",
        "    # Print the chatbot response and citations\n",
        "    chatbot_response = \"\"\n",
        "    print(\"\\nChatbot:\")\n",
        "\n",
        "    for event in response:\n",
        "        if event.event_type == \"text-generation\":\n",
        "            print(event.text, end=\"\")\n",
        "            chatbot_response += event.text\n",
        "        if event.event_type == \"stream-end\":\n",
        "            if event.response.citations:\n",
        "                print(\"\\n\\nCITATIONS:\")\n",
        "                for citation in event.response.citations:\n",
        "                    print(citation)\n",
        "            if event.response.documents:\n",
        "                print(\"\\nCITED DOCUMENTS:\")\n",
        "                for document in event.response.documents:\n",
        "                    print(document)\n",
        "            # Update the chat history for the next turn\n",
        "            chat_history = event.response.chat_history\n",
        "\n",
        "    return chat_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "XU7oWbO_p9s7",
        "outputId": "f9033b9f-924a-41ea-f330-aff16feb1c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chatbot:\n",
            "Hello! I'm here to help. Please go ahead and ask your question, and I'll do my best to provide a helpful and informative answer."
          ]
        }
      ],
      "source": [
        "# Turn # 1\n",
        "chat_history = run_chatbot(\"Hello, I have a question\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XmcP1wxosMFe",
        "outputId": "f4fe1258-e765-48f6-a5a9-7b7bf3968f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chatbot:\n",
            "**Prompt engineering** is the practice of designing and optimizing input prompts to guide language models, like GPT, to generate desired outputs. It involves crafting specific, clear, and contextually rich instructions or questions to elicit accurate, relevant, or creative responses from AI systems.\n",
            "\n",
            "Here’s a breakdown of key aspects of prompt engineering:\n",
            "\n",
            "1. **Purpose**:  \n",
            "   - To improve the quality, relevance, and specificity of AI-generated responses.  \n",
            "   - To align the model's output with the user's intent or task requirements.\n",
            "\n",
            "2. **Techniques**:  \n",
            "   - **Clear Instructions**: Providing explicit directions or examples in the prompt.  \n",
            "   - **Contextual Information**: Adding background details to guide the model.  \n",
            "   - **Iterative Refinement**: Testing and adjusting prompts to improve results.  \n",
            "   - **Role Assignment**: Framing the model as a specific \"role\" (e.g., \"Act as a teacher\").  \n",
            "   - **Constraints**: Limiting the scope of the response (e.g., \"Answer in 3 sentences\").  \n",
            "\n",
            "3. **Applications**:  \n",
            "   - **Content Generation**: Writing articles, stories, or code.  \n",
            "   - **Problem Solving**: Answering questions or solving mathematical problems.  \n",
            "   - **Creative Tasks**: Generating ideas, art descriptions, or dialogue.  \n",
            "   - **Data Extraction**: Pulling specific information from text.  \n",
            "\n",
            "4. **Challenges**:  \n",
            "   - **Ambiguity**: Vague prompts can lead to irrelevant or incorrect responses.  \n",
            "   - **Bias**: Poorly designed prompts may amplify biases in the model.  \n",
            "   - **Complexity**: Balancing detail and simplicity in prompts.  \n",
            "\n",
            "Prompt engineering is particularly important for large language models (LLMs) because their outputs are highly sensitive to the input they receive. It’s both an art and a science, requiring creativity, experimentation, and understanding of how the model processes information."
          ]
        }
      ],
      "source": [
        "# Turn # 2\n",
        "chat_history = run_chatbot(\"What is prompt engineering?\", chat_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EMXWrOJmsPDZ",
        "outputId": "57ce06bb-5d08-4e4c-83e5-fe3191512277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Chatbot:\n",
            "**Zero-shot** and **few-shot prompting** are techniques used in prompt engineering to guide language models, but they differ in how they leverage examples or context. Here's the breakdown:\n",
            "\n",
            "---\n",
            "\n",
            "### **Zero-Shot Prompting**\n",
            "- **Definition**: Zero-shot prompting involves giving the model a task or question **without providing any examples** of the desired output. The model relies solely on its pre-trained knowledge to generate a response.\n",
            "- **Example**:  \n",
            "  Prompt: *\"Translate the following English sentence into French: 'The cat is on the mat.'\"*  \n",
            "  The model generates the translation based on its understanding of English and French, without seeing any prior examples.\n",
            "- **Use Case**: Ideal when the model is expected to generalize well to new tasks or when examples are not available.\n",
            "- **Advantage**: Simple and requires no additional data.\n",
            "- **Limitation**: May produce less accurate or inconsistent results for complex or ambiguous tasks.\n",
            "\n",
            "---\n",
            "\n",
            "### **Few-Shot Prompting**\n",
            "- **Definition**: Few-shot prompting involves providing the model with a **few examples** of input-output pairs before asking it to perform the task. These examples act as a guide for the model to understand the expected format or style of the response.\n",
            "- **Example**:  \n",
            "  Prompt:  \n",
            "  *\"Translate the following English sentences into French:  \n",
            "  1. English: 'The dog is in the yard.' → French: 'Le chien est dans la cour.'  \n",
            "  2. English: 'She reads a book.' → French: 'Elle lit un livre.'  \n",
            "  3. English: 'The cat is on the mat.' → French: '*Le chat est sur le tapis.*'\"*  \n",
            "  The model uses the examples to infer the correct translation pattern.\n",
            "- **Use Case**: Useful when the task requires specific formatting, style, or reasoning that the model might not infer without examples.\n",
            "- **Advantage**: Often improves accuracy and consistency by providing context.\n",
            "- **Limitation**: Requires carefully selected examples and may be more resource-intensive.\n",
            "\n",
            "---\n",
            "\n",
            "### **Key Differences**\n",
            "| **Aspect**            | **Zero-Shot**                          | **Few-Shot**                          |\n",
            "|------------------------|----------------------------------------|---------------------------------------|\n",
            "| **Examples Provided**  | None                                   | A few examples                        |\n",
            "| **Reliance on Context**| Depends entirely on pre-trained knowledge | Uses examples to guide the response   |\n",
            "| **Complexity**         | Simpler to implement                   | Requires careful example selection    |\n",
            "| **Accuracy**           | May be lower for complex tasks         | Often higher due to guidance          |\n",
            "\n",
            "---\n",
            "\n",
            "### **When to Use Which?**\n",
            "- Use **zero-shot** when the task is straightforward or when you want to test the model's inherent capabilities.  \n",
            "- Use **few-shot** when the task requires specific formatting, reasoning, or when you want to improve the model's performance with minimal examples.\n",
            "\n",
            "Both techniques are powerful tools in prompt engineering, and the choice depends on the task and available resources."
          ]
        }
      ],
      "source": [
        "# Turn # 4\n",
        "chat_history = run_chatbot(\"What's the difference between zero-shot and few-shot prompting\", chat_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B29zSJpVGh5k"
      },
      "source": [
        "---\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
